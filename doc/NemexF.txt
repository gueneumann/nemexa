GN, 2014 - my comments on NEMEX_F


MAY, 2014:

Start implementing Faerie as part of nemex.f. Already implemented by Amir:
- inverted index structure for Faerie for char and token based approach
- nemex-f configuration under /nemexa/src/main/webapp/resources/configurations.xml
	propertye "nemex-f"
	
- adapt/extend xml configuration 
	- pruning function
	- char/token
	- output function

Steps to do:
- define class NemexFIndex
	- keeps the dictionary and its main parameters: sim funct, ngram etc.
	- load and store gazetteer as done in NEMEX_A
		- note: it is required that inverted list of a ngram-key are naturally ordered, 
					which seems to be the case
	
- then define class NemexFContainer.java
	- mantains the state of an extraction session

	ngram-list: ngram representation of input text; 
		can use type de.dfki.lt.nemex.a.ngram.CharacterNgram
	ngram-list-length: 
		is defined by above
  	inverted-lists: a list of lists of Long or NIL
  		index follows ngram-list
  	position-list: actually a hash
  		a mapping between top elements of inverted-lists and their position in the ngram-list 
  		this actually indicates which ngrams of an entry of the dictionary are aligned in the input ngram list
  	min-heap
  		these top-elements are also stored in the min heap
  		
  		- find java library for Fibonacci min heap -> 
			I think import java.util.PriorityQueue; 
			should do it because it already implements a min-heap as default
			see also how it is used in JTIG
	
	
  	toplevel-element
  		the poped element of the heap
  	count-array
  		the array that records an entity's occurrence number for each possible substring of the document
  		in my Lisp version I am using a hashmap from spans to integers which records the occurrence of an entity
  		e at all positions in ngram-list
  		thus the spans correspond to candidate substrings in the input that eventually are similar with e
  	valid-substring-interval
  		store the lower/upper bounds for current entity
  		has to be computed online with the used sim function
  	lower-entity-length
  		T_l
  		has to be computed online with the used sim function
 
 
 - define NemexFController
 	- define major search strategies:
 		lazy-count-pruning
		initialize-entity-boundaries
		call BUCKET or BINARY pruning
		get-candidate-pairs
 		
 - define NemexFMain:
 	initialization 
 		dictionary
 		document
 		container
 	main loop over heap: 
 	- initialization of NEMEX_F object for given input text
 	- main loop over heap: for each selected entity e do:
 		- initialization of count-array
 		- main steps then
 			lazy-count-pruning
			initialize-entity-boundaries
			call BUCKET or BINARY pruning
			get-candidate-pairs
			adjust NEMEX_F object
		- adjust NEMEX_F means
			- delete e from inverted lists
			- select new top level elements
			- create or update position lists
			
25th AUGUST, 2014:

Start implementation of Faerie.

For the ngram representation of the input I need to make sure that the ngram list is position preserving.
Thus, I need to use /nemexa/src/main/java/de/dfki/lt/nemex/a/ngram/CharacterNgramWithDuplicate.java
And hence make sure that <ignoreDuplicateNgrams>false</ignoreDuplicateNgrams> !

In LispVersion I preprocess the ngram-list and its length is stored in internal entry
In NemexA this is NOT done. So, I need to compute the ngram-lenght of an entry online ! 
It is possible, but check equation.

SOLUTION:
In general, a string x consisting of |X| letters yields (|x| + n − 1) elements of ngrams, cf. Coling 2010.
However, I need to substract additional ( 2 * (n - 1)), because I do not use start/end ngram.
(|x| + (n − 1) - ( 2 * (n - 1))) = (|x| - n + 1)

OBACHT:
IDs of lexical entries start from 1 in the dictionary, but are stored as ID-1 in the inverted index.
So, make sure to retrieve entries in the gazetters via invertedIndexElement+1!

NOTE:
I will start with 0 for document ! (different as in paper)

NOTE:
in paper document starts at index i=1, and len=l
in my implementation: i=0, len=l+1

SIMILARITY FUNCTIONS:
Have to be reimplemented because NemexA is tuned for input ngram only; 
it means it access entityLength from ngram size when it constructs index, but then does not store it with entity.
Simialrity functions compute basically for online-constructed ngrams
But for Faerie ngram length have to be offline !
Muss also package kopieren und anpassen

NOTE:
I do the initialization of the entity boundaries before calling the pruning strategies.

NOTE:
I explicitly avoid duplicate entities in minHeap, because otherwise, duplicate candidates are created.
I think this is correct, because the position list takes care of the occurrence of ngrams of an entity
in a string.

NOTE on sorting:
position list must be sorted, but adjustNemexFContainer does not guarantee it; so make sure pruner is doing

NOTE:
I should precompute ngram length of dictionary entries when loading a dictionary and
create a hashmap. This would allow me to remove entity content from memory, and only store index and length;

Problems:

current :no version crashes for large examples.
It seems that inverted index list keeps some elements at least twice.
It seems to be caused by removing position list of top element !
	Without removing it works, but it seems to be not exhaustive.
	Removal here is bad because I do not sort ! So need to do sort, because
	for pruning I have to do it anyway
A single entry as query is not found. 
	-> caused by adjustment of container, because cause to early termination
	-> also had to adapt initialContainer
Now, it seems that get candidates is too restricted if threshold is > 0.9
because then ngramLen=min=max -> check this case
seems to be ok in getCandidates, where I use >= instead of >

OK< let me first print the found pairs, so that I can see whether results are correct !
	-> Take care about proper computation of right span!
Then, item counter and candidate counter with time.
	-> keep an item counter for the spanHash in countOccurrenceOfentityAtithPosition
	
9th SEPTEMBER, 2014: 

first version for :no and :lazy cases implemented and tested

1. define pruning interface -> OK
2. integrate no and lazy strategies -> OK
3. Initialize aligner when creating the controller object -> OK

4. similarity functions

- cosine -> OK -> double-checked
- dice -> OK -> double-checked
- jaccard -> OK -> double checked
- ed OK -> threshold TAU should be a double encoding 
			an integer which actually encodes the number of allowable edit operations
			HIERIX: results look a bit strange
		-> double-checked
- eds -> OK -> -> double-checked


18th SEPTEMBER, 2014:
5. bucket -> OK

21th SEPTEMBER, 2014:
6. binary -> OK
	- first version runs
	- got same pairs as for bucket for small documents
	- but too many for more complex examples 
	- also, for small documents computes more items than for bucket, which should not be the case
	-> Do I count too much ?
	
-> check it again carefully ->
	- if I do not use binaryInputShift, I get the same result when just using i=i+1
		- I guess it means that binaryInputShift is OK
		
-> I do not see what is wrong with binary, maybe here
		de.dfki.lt.nemex.f.aligner.BinaryCountPruneAligner.enumerateCandidateWindows(List<Integer>, NemexFContainer)
		where I could do a more restricted min/max for enumerating candidates;
		however, there are cases were distance is lower than LO for where it is not with bucket.

		
29th SEPTEMBER
- implemented tighterUpperWindowSize() for similarity function interface
	for dice, jaccard, cosine, see 2012-Faerie page 534
- use NemexFContainer.computeUpperWindowSize as interface to the sim fct for selecting best upper bound fct.

8th OCTOBER
- 	when running binary count with different sim fct, I get different result pairs, 
	for Jaccard and EDS compared to Cosine and Dice
	
-> correct - yes, should be because different definition of similarity fcts

MOVE nemexa to gitHub ?

7. improve memory footprint for dictionary
	- currently I need to load whole word-strings because I compute ngram length of entities online
	
	- add it to dictionary when loading, and then remove information not needed to keep in memory
		- keep own hash table with mapping ID -> ngramLength
		- build when dictionary is loaded; should also be possible to make it persisted

8. IMPROVEMENT:

MAY, 2015

- post filtering of "wrong" substrings, i.e., matched substrings that are in the wrong context, e.g.,
	- "present" -> "ent" matches because it is a dictionary entry
	- how to "learn" post-filters ?
	
- foundCandidates: (check whether correct)
	- it is an inverted index to entities e
	- collections of start positions
		- collections of length positions
	- for that reason: e should be one single element
		- start and len are key sets
		- CHANGED MAY 2015
	
- ranking of candidates
	- according to paper page 536, advantage of binary pruning
		- the best similar pairs are those which share as many tokens as possible
	  	- group all candidates based on the same number of tokens
	  	- consider a group G_g with g tokens
	  	- let T_g be a threshold computed on basis of |e| and g
	  		 -> How exactly is this computed ?
	  	- then we prune all elements in group G_g, if |P_e[pi, ..., pj]| < T_g
	- I think, I have that as part of my foundCandidates

- selecting best matches:
	- note, that under index left->[len_i, ..., len_j], I find same entity e !
	- instead of returning each substring [left, len_i] individually, I can return substring:
		- [left, len_j] -> gives max length
		- [left, floor(len_j, len_i)] -> gives middle length -> most closest match
	-> DONE
		
	- if there is a chain/bucket of left elements left_1,left_2,..,leftK...
		- [left1, max-len-element(left_1,left_2,..,leftK)] 
			-> gives maximum window size of entity e in document
	-> TODO: length still not computed correctly, because I have to consider distance between 
		start and max len of a bucket to get the max len
	-> DONE
	- similar for middle left-element
		- [floor(left_1,left_2,..,leftK), middle-len-element(left_1,left_2,..,leftK)] 
	-> DONE

- DONE: check and update data structure foundCandidates
	-> List(Long) is not required

- DONE: Integration of LOG file


- DONE: define specific class for via CandidateSelector

-> QUESTION: can one compute CandidateSelector online ?

MAY, 2015:

- DONE: define resetting of controller
	- controllers can now run in parallel on same dictionary and ngram of input string
		-> NOTE REALLY, because single global NemexFDataObject binds aligner and selector
		-> check what slots of NemexFDataObject are really necessary
-> DONE: defined NemexFBean for us instead of static NemexFDataObject
	-> Should now be possible to run NemexF on a cluster for same dictionary and distributed file
	
- verification:
	Still to be done ?
	According to an email to Guoliang Li:
	I have a further question concerning the verification process. Assuming we have edit distance ED and threshold t=2:
	Does verification mean that for a candidate pair s and e, one has finally to show that ED(s,e) <= 2, 
	because so far one only has shown that a minimal overlap holds?
	HE says: YES !
	
->	BUT according to Jiang, VLDB, 2014: I think
	it is realized through the merge-based algorithm using overlap, which I have
	NemexA: de.dfki.lt.nemex.a.similarity.ApproximateStringSimilarityImpl.doApproximateStringMatchingUsingCPMerge(double)
	NemexF: de.dfki.lt.nemex.f.NemexFContainer.enumerateCandidatePairsMain(List<Integer>)

-> I consider it as being DONE

HIERIX May 2015:

- define mapping entity_id -> length of entity in order to avoid storing/computing length online

- compare directly the resulting set of two pruners so to see the difference or at least provide a sorted output
-> sorted according to span?

- evaluation:
	
	- for each matched NE, I could also monitor the number of items, pairs etc.
		- currently I do it only globally
	- for example in de.dfki.lt.nemex.f.NemexFContainer.adjustNemexFContainer(Long)
	
	- basically: create graphs like Dong group is doing
	- see their data sources

-> could it make sense to keep a hash of successful NE-matches to spans anyway
	because then selection of candidates could be easier ?
	or the identification of new pruning strategies?
	
- improve lexicon storage -> will create own NEMEXF gazetter class

- make own project ?

- can I use better data structure than hashMap ? 


- why does binary bucket compute more candidate pairs than bucket and lazy 
	-> need to do automatic evaluation to compare results
	-> do this, once I have implemented my post-filtering
-> with post-filtering it looks much better, especially when using MiddleBucket
-> anyway: it seems I am computing too many candidates; maybe binary branch is wrong at some point

MAY, 2015: note the main difference seems to be that enumerateCandidatePairs
	is called differently:
		- for binary: de.dfki.lt.nemex.f.aligner.BinaryCountPruneAligner.enumerateCandidateWindows(List<Integer>, NemexFContainer)
		- for others: de.dfki.lt.nemex.f.NemexFContainer.enumerateCandidatePairsMain(List<Integer>)
	-> I guess that because of this, counting is performed too often
	-> but it also returns fewer NE with smaller threshold ???
	-> I have integrated own tests in binaryBucket so check correctness of these !!!
	
-> MAYBE the problem is when dividing the position list/substring ?

-> think about own bucket strategy or similar ! then I might understand better how the stuff is working :-)

- other similarity functions
- demo-page which shows how NemexF is running
- restful API and showing streaming based extraction

ERROR ?

when I use this
NemexFDataObject.nGramSize = 1;
NemexFDataObject.similarityMeasure = SimilarityMeasure.ED_SIMILARITY_MEASURE;
NemexFDataObject.similarityThreshold = 0.0; // means, no edit operations :-)

I get strange results, e.g., [36,46]: [3]; membership; [-9.197762, venkatesh, NG:1:-9.197762]
-> Check, what is going on, when n-gram=1 !
-> same problem with cosine similarity and threshold=1.0
-> looks like a problem with ngram=1 

-> it seems that de.dfki.lt.nemex.f.data.NemexFIndex.getInvertedIndexAndCopy(String, String) is wrong for ngram=1
-> if a word has same ngram at n-different positions, it will be selected n-times as independent match !
	e.g., ngram "o" matches "abdominoperineal" at position 4 and 8 -> then it will occur two times in the inverted list
	-> is this correct ?
	-> IT can happen in general for all ngrams, but more likely for smaller onces, especially, ngram=1
-> this seems to happen when building the invertedNgram
-> it seems  to be caused by de.dfki.lt.nemex.a.ngram.CharacterNgramWithDuplicate.CharacterNgramWithDuplicate(String, int):
	- when building the inverted, I should NOT add double entries into inverted index of an ngram at certain position
	-> I added test in de.dfki.lt.nemex.f.data.InvertedList_FAERIE_charBased.InvertedList_FAERIE_charBased(Gazetteer, int, boolean)
	-> Correct ? not yet, but gets fewer elements
	-> so, DO I build inverted index correctly ? and compute ngrams correctly?
-> DO I USE indexing of sublist (starting from 0) and pos-list (starting from 1) correct ? in all aligners etc.? 

GN, April 2019:

I think, in case of ED and EDS I can use the ngram index, but in case of the others, I should use
token-based index. See Faerie paper !!!

Thus, I need to parameterize NemexF in this respect when
- creating the index
- creating the heap

Thus, when I change the similarity function I need to recompute the inverted index of the dictionary.

And then it might be the case that I need a final verify check at least for ED and EDS using Palign because of CPmerge for token level

Thus, what I have so far is basically a version were I use for ALL similarity function N-GRAMs

Possible changes in code:
- de.dfki.lt.nemex.f.data.InvertedList_FAERIE_tokenBased.InvertedList_FAERIE_tokenBased(Gazetteer, int, boolean)
	make sure to use # if used for splitting MLW entries





